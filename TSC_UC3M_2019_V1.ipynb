{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSC_UC3M_2019  \n",
    "\n",
    "Author: Jon Lérida  \n",
    "Description: The following notebook aims to cover the first steps in the word embeddings classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "from time import monotonic as timer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk import download\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sys\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the FastText file  \n",
    "This file contains the whole Wikipedia vocabulary in embedding format (about 6 Gb). Each word is mapped to a high dimension (i.e, 300) vector. So the final matrix will be an $N \\times 300$, where $N$ is the number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (mins): 7.24\n"
     ]
    }
   ],
   "source": [
    "model_path = 'wiki_data/wiki.en.vec'\n",
    "start = timer()\n",
    "wv = KeyedVectors.load_word2vec_format(model_path)\n",
    "\n",
    "print('Elapsed time (mins): {:.2f}'.format((timer() - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2519370 tokens with size 300\n"
     ]
    }
   ],
   "source": [
    "print(\"The model has %s tokens with size %s\" % (len(wv.vocab), wv.vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_wv_ variable contains the full file (_ie_, all the embeddings) as a dictionary list, so we can do things like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-0.092271, -0.14855 , -0.14696 ,  0.013   , -0.40305 , -0.31004 ,\n",
      "        0.1022  , -0.42087 , -0.22948 ,  0.12853 ], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pprint(wv['car'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the first 10 components of the _car_ vector representation. Also, _KeyedVectors_ module allows us to check for word similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leri/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: cars similarity score: 0.8341586589813232\n",
      "Word: automobile similarity score: 0.7181168794631958\n",
      "Word: truck similarity score: 0.7055484056472778\n",
      "Word: motorcar similarity score: 0.6987981796264648\n",
      "Word: vehicle similarity score: 0.6951144337654114\n",
      "Word: driver similarity score: 0.6925972700119019\n",
      "Word: drivecar similarity score: 0.6851067543029785\n",
      "Word: minivan similarity score: 0.6729590892791748\n",
      "Word: roadster similarity score: 0.6720188856124878\n",
      "Word: racecars similarity score: 0.6717766523361206\n"
     ]
    }
   ],
   "source": [
    "for word_sim in wv.similar_by_word('car'):\n",
    "    print(\"Word:\", word_sim[0], \"similarity score:\", word_sim[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download some Wikipedia definitions  \n",
    "\n",
    "This trial will include _Mathematics, Economics, Psicology_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting definition of  Mathematics\n",
      "Getting definition of  Economics\n",
      "Getting definition of  Philosophy\n",
      "Getting definition of  Art\n",
      "Number of Documents:  4\n"
     ]
    }
   ],
   "source": [
    "# Create an instance\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "categories = ['Mathematics', 'Economics', 'Philosophy', 'Art']\n",
    "\n",
    "docs = []\n",
    "\n",
    "# Include the category definition as a list\n",
    "for cat in categories:\n",
    "    print(\"Getting definition of \", cat)\n",
    "    docs.append({cat: wiki_wiki.page(cat).text})\n",
    "\n",
    "print(\"Number of Documents: \", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus acquisition  \n",
    "Using the _gensim_ Python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Mathematics\n",
      "Tokenizing Economics\n",
      "Tokenizing Philosophy\n",
      "Tokenizing Art\n",
      "\n",
      "Done\n",
      "\n",
      "Cleaning corpus Mathematics\n",
      "Cleaning corpus Economics\n",
      "Cleaning corpus Philosophy\n",
      "Cleaning corpus Art\n",
      "\n",
      "Dictionary w/o numbers contains 4726 unique tokens\n",
      "\n",
      "First terms in the dictionary (not by frequency):\n",
      "0 : 10th\n",
      "1 : 16th\n",
      "2 : 17th\n",
      "3 : 18th\n",
      "4 : 1930s\n",
      "5 : 19th\n",
      "6 : 20th\n",
      "7 : 2nd\n",
      "8 : 3rd\n",
      "9 : 6th\n",
      "10 : 9th\n",
      "11 : abel\n",
      "12 : abstract\n",
      "13 : abstraction\n",
      "14 : abstractness\n",
      "\n",
      "\n",
      "Filtering dictionary...\n",
      "The dictionary contains 4448 terms\n",
      "First terms in the dictionary:\n",
      "0 : 10th\n",
      "1 : 16th\n",
      "2 : 17th\n",
      "3 : 18th\n",
      "4 : 1930s\n",
      "5 : 2nd\n",
      "6 : 3rd\n",
      "7 : 6th\n",
      "8 : 9th\n",
      "9 : abel\n",
      "The dictionary contains 4448 terms\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "corpus_clean = []\n",
    "\n",
    "for doc in docs:\n",
    "    for val in doc.values():\n",
    "        print(\"Tokenizing\", list(doc.keys())[0])\n",
    "        # Tokenize each text entry. \n",
    "        tokens = word_tokenize(val)\n",
    "        tokens_filtered = [el.lower() for el in tokens if el.isalnum()]\n",
    "        tokens_lemmatized = [wnl.lemmatize(el) for el in tokens_filtered]\n",
    "        tokens_clean = [token for token in tokens_lemmatized if token not in stopwords_en]    \n",
    "        corpus_clean.append(tokens_clean)\n",
    "        \n",
    "\n",
    "print(\"\\nDone\\n\")\n",
    "# Delete digit tokens\n",
    "corpus_clean_no_number = []\n",
    "for n, corpus in enumerate(corpus_clean):\n",
    "    print(\"Cleaning corpus\", list(docs[n].keys())[0])\n",
    "    corpus_clean_no_number.append([x for x in corpus if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())])\n",
    "    \n",
    "    \n",
    "    \n",
    "# Creamos el diccionario de tokens y eliminamos los números\n",
    "D = gensim.corpora.Dictionary(corpus_clean_no_number)\n",
    "n_tokens = len(D)\n",
    "print(\"\\nDictionary w/o numbers contains\", len(D), \"unique tokens\")\n",
    "\n",
    "print('\\nFirst terms in the dictionary (not by frequency):')\n",
    "for n in range(15):\n",
    "    print(str(n), ':', D[n])\n",
    "    \n",
    "    \n",
    "no_below = 1\n",
    "no_above = .75 \n",
    "D.filter_extremes(no_below=no_below, no_above=no_above, keep_n=500000)\n",
    "n_tokens = len(D)\n",
    "print(\"\\n\\nFiltering dictionary...\")\n",
    "print('The dictionary contains', n_tokens, 'terms')\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])\n",
    "    \n",
    "    \n",
    "print('The dictionary contains', n_tokens, 'terms')    \n",
    "\n",
    "\n",
    "corpus_bow = [D.doc2bow(doc) for doc in corpus_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have a dictionary ($D$) which contains all the tokens used in the definitions  \n",
    "\n",
    "additionally, each article (definition) has been converted to a sparse vector, where each position contains a tuple formed by (word_index, frequency). This way the memory usage has been reduced, since only few entries per article are stored in memory. For exampe, for definition _Mathemathics_, the first 10 terms are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_bow[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing, we have downloaded some Wikipedia definitions and saved them as token vectors (_i.e_, each document is a list of words). After that, some language proccesing has been made. Finally, the document is represented as a sparse vector, which contains the token indexes and its frequency in the given article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the vector representation of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 components..\n",
      " [ 0.17572  0.3706  -0.31275  0.289    0.17087  0.39907  0.14805 -0.34401\n",
      " -0.3345   0.41444]\n"
     ]
    }
   ],
   "source": [
    "embedding = []\n",
    "for n, index in enumerate(D):\n",
    "    token = D[index]\n",
    "    try:\n",
    "        embedding.append(wv[token])\n",
    "    except KeyError:\n",
    "        embedding.append(np.array([np.nan]*wv.vector_size))\n",
    "        \n",
    "        \n",
    "print(\"First 10 components..\\n\", embedding[10][0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN number: 0\n",
      "Number of stored tokens 4404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.074886</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>-0.20421</td>\n",
       "      <td>-0.088999</td>\n",
       "      <td>-0.074444</td>\n",
       "      <td>0.15969</td>\n",
       "      <td>0.286930</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.018306</td>\n",
       "      <td>0.48267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021786</td>\n",
       "      <td>0.251020</td>\n",
       "      <td>0.306690</td>\n",
       "      <td>-0.164790</td>\n",
       "      <td>0.36931</td>\n",
       "      <td>0.078496</td>\n",
       "      <td>-0.012397</td>\n",
       "      <td>0.052154</td>\n",
       "      <td>0.169010</td>\n",
       "      <td>abel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.175720</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>-0.31275</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.170870</td>\n",
       "      <td>0.39907</td>\n",
       "      <td>0.148050</td>\n",
       "      <td>-0.344010</td>\n",
       "      <td>-0.334500</td>\n",
       "      <td>0.41444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065901</td>\n",
       "      <td>0.034094</td>\n",
       "      <td>0.250570</td>\n",
       "      <td>0.258220</td>\n",
       "      <td>-0.42897</td>\n",
       "      <td>-0.175850</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>0.088724</td>\n",
       "      <td>-0.051181</td>\n",
       "      <td>abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.028491</td>\n",
       "      <td>0.330540</td>\n",
       "      <td>-0.62929</td>\n",
       "      <td>0.255510</td>\n",
       "      <td>0.165470</td>\n",
       "      <td>0.37520</td>\n",
       "      <td>0.283960</td>\n",
       "      <td>-0.361090</td>\n",
       "      <td>-0.430010</td>\n",
       "      <td>0.54781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169950</td>\n",
       "      <td>0.041594</td>\n",
       "      <td>0.267230</td>\n",
       "      <td>0.395550</td>\n",
       "      <td>-0.25651</td>\n",
       "      <td>-0.267780</td>\n",
       "      <td>0.218250</td>\n",
       "      <td>0.367960</td>\n",
       "      <td>0.108910</td>\n",
       "      <td>abstraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045932</td>\n",
       "      <td>0.248930</td>\n",
       "      <td>-0.52394</td>\n",
       "      <td>0.467150</td>\n",
       "      <td>0.103930</td>\n",
       "      <td>0.23662</td>\n",
       "      <td>0.146050</td>\n",
       "      <td>-0.233570</td>\n",
       "      <td>-0.243330</td>\n",
       "      <td>0.31687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084081</td>\n",
       "      <td>0.075570</td>\n",
       "      <td>0.037863</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>-0.46469</td>\n",
       "      <td>-0.239770</td>\n",
       "      <td>0.180290</td>\n",
       "      <td>0.240790</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>abstractness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.309770</td>\n",
       "      <td>0.158330</td>\n",
       "      <td>-0.19837</td>\n",
       "      <td>0.447750</td>\n",
       "      <td>-0.303320</td>\n",
       "      <td>-0.22506</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>-0.230270</td>\n",
       "      <td>-0.046159</td>\n",
       "      <td>0.21150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164680</td>\n",
       "      <td>0.037584</td>\n",
       "      <td>-0.111410</td>\n",
       "      <td>-0.012267</td>\n",
       "      <td>-0.13096</td>\n",
       "      <td>-0.094362</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.626650</td>\n",
       "      <td>-0.489620</td>\n",
       "      <td>accelerating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1        2         3         4        5         6  \\\n",
       "0 -0.074886  0.038086 -0.20421 -0.088999 -0.074444  0.15969  0.286930   \n",
       "1  0.175720  0.370600 -0.31275  0.289000  0.170870  0.39907  0.148050   \n",
       "2 -0.028491  0.330540 -0.62929  0.255510  0.165470  0.37520  0.283960   \n",
       "3  0.045932  0.248930 -0.52394  0.467150  0.103930  0.23662  0.146050   \n",
       "4 -0.309770  0.158330 -0.19837  0.447750 -0.303320 -0.22506  0.005589   \n",
       "\n",
       "          7         8        9      ...            291       292       293  \\\n",
       "0  0.002115  0.018306  0.48267      ...      -0.021786  0.251020  0.306690   \n",
       "1 -0.344010 -0.334500  0.41444      ...      -0.065901  0.034094  0.250570   \n",
       "2 -0.361090 -0.430010  0.54781      ...      -0.169950  0.041594  0.267230   \n",
       "3 -0.233570 -0.243330  0.31687      ...      -0.084081  0.075570  0.037863   \n",
       "4 -0.230270 -0.046159  0.21150      ...       0.164680  0.037584 -0.111410   \n",
       "\n",
       "        294      295       296       297       298       299         Token  \n",
       "0 -0.164790  0.36931  0.078496 -0.012397  0.052154  0.169010          abel  \n",
       "1  0.258220 -0.42897 -0.175850 -0.012072  0.088724 -0.051181      abstract  \n",
       "2  0.395550 -0.25651 -0.267780  0.218250  0.367960  0.108910   abstraction  \n",
       "3  0.460900 -0.46469 -0.239770  0.180290  0.240790  0.010810  abstractness  \n",
       "4 -0.012267 -0.13096 -0.094362  0.286900  0.626650 -0.489620  accelerating  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(embedding)\n",
    "\n",
    "# Add token column\n",
    "df['Token'] = [D[index] for index in D]\n",
    "# Delete NaN entries (i.e, tokens which have not vector representation)\n",
    "df.dropna(inplace = True)\n",
    "df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "print(\"NaN number:\", df.isnull().sum().sum())\n",
    "print(\"Number of stored tokens\", df.shape[0])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model as a KeyedVector object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, next time we want to work with the same corpus data it won't be necessary to repeat the whole proccess (which is slow, since big files are stored in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus succesfully saved in the object\n"
     ]
    }
   ],
   "source": [
    "# Create an empty model\n",
    "wv_corpus = KeyedVectors(vector_size= wv.vector_size)\n",
    "\n",
    "# Fill with the dataframe corpus\n",
    "wv_corpus.add(entities= df.Token, weights= df.loc[:, range(wv.vector_size)])\n",
    "\n",
    "print(\"Corpus succesfully saved in the object\")\n",
    "\n",
    "# Save in disk storage\n",
    "wv_corpus.save('TSC_Corpus1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
