{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "from time import monotonic as timer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk import download\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sys\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "# import sim_graph0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load('./TSC_Corpus1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens on this corpus: 4404\n",
      "The first n tokens are  ['abel', 'abstract', 'abstraction', 'abstractness']\n"
     ]
    }
   ],
   "source": [
    "tokens = list(wv.vocab.keys())\n",
    "print(\"Number of tokens on this corpus:\", len(tokens))\n",
    "print(\"The first n tokens are \", tokens[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIALS  \n",
    "\n",
    "## The following code should be used as an external Python module. This is just for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from time import time\n",
    "\n",
    "from scipy.sparse import csr_matrix, identity, diags, issparse\n",
    "# This is not being used, because it is quit slow\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "from sklearn.neighbors import radius_neighbors_graph\n",
    "\n",
    "EPS = np.finfo(float).tiny\n",
    "\n",
    "\n",
    "class SimGraph(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Generic class to generate similarity graphs from data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Tg,  out_path=None, label=\"dg\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Stores the main attributes of a datagraph object and loads the graph\n",
    "        data as a list of node attributes from a database\n",
    "        \"\"\"\n",
    "\n",
    "        # #########################################\n",
    "        # Internal name assigned to the graph object\n",
    "        self.label = label\n",
    "\n",
    "        # ###############\n",
    "        # Graph variables\n",
    "        self.Tg = Tg                       # Data matrix\n",
    "        self.n_nodes, self.dim = Tg.shape  # Data dimensions\n",
    "\n",
    "        # ###############\n",
    "        # Other variables\n",
    "\n",
    "        # Path for the output files\n",
    "        self.out_path = out_path\n",
    "\n",
    "        #\n",
    "        self.edgeT_id = None   # List of edges, as pairs (i, j) of indices.\n",
    "        return\n",
    "\n",
    "    def computeGraph(self, R=None, similarity='He', g=1, th_gauss=0.1):\n",
    "        \"\"\"\n",
    "        Computes a sparse graph for the self graph structure.\n",
    "        The self graph must containg a T-matrix, self.T\n",
    "\n",
    "        Inputs:\n",
    "            :self.T:   Data matrix\n",
    "            :R: Radius. Edges link all data pairs at distance lower than R\n",
    "                This is to forze a sparse graph.\n",
    "            :similarity: Similarity measure used to compute affinity matrix\n",
    "                Available options are:\n",
    "                    'l1'     :1 minus l1 distance\n",
    "                    'He'     :1 minus squared Hellinger's distance (JS)\n",
    "                              (sklearn-based implementation)\n",
    "                    'Gauss'  :An exponential function of the squared l2\n",
    "                              distance\n",
    "            :g: Exponent for the affinity mapping (not used for 'Gauss')\n",
    "            :th_gauss:  Similarity threshold All similarity values below this\n",
    "                threshold are set to zero. This is only for the gauss method,\n",
    "                the rest of them compute the threshold automatically from R).\n",
    "\n",
    "        Returns:\n",
    "            :self.edgeT_id:  List of edges, as pairs (i, j) of indices\n",
    "            :self.affinityT: List of affinity values for each pair in edgeT_id\n",
    "            :self.df_edges:  Pandas dataframe with one row per edge and columns\n",
    "                'Source', 'Target' and 'Weihgt'. The weight is equal to the\n",
    "                (mapped) affinity value\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"-- Computing graph with {self.n_nodes} nodes\")\n",
    "        logging.info(f\"-- Similarity measure: {similarity}\")\n",
    "\n",
    "        # #########################\n",
    "        # Computing Distance Matrix\n",
    "\n",
    "        # This is just to abbreviate\n",
    "        Tg = self.Tg\n",
    "\n",
    "        # Select Distance measure for radius_neighbor_graph\n",
    "        if similarity in ['Gauss', 'He']:\n",
    "            d = 'l2'     # Note: l2 seems equivalent to minkowski (p=2)\n",
    "        elif similarity in ['l1']:\n",
    "            d = 'l1'     # Note: l1 seems equivalent to manhattan\n",
    "        else:\n",
    "            logging.error(\"computeTsubGraph ERROR: Unknown similarity measure\")\n",
    "            exit()\n",
    "\n",
    "        # Select secondary radius\n",
    "        R0 = R\n",
    "\n",
    "        # Compute the connectivity graph of all pair of nodes at distence\n",
    "        # below R0\n",
    "        # IMPORTANT: Note that, despite radius_neighbors_graph has an option\n",
    "        # 'distance' that returns the distance values, it cannot be used in\n",
    "        # any case because the distance matrix does not distinghish between\n",
    "        # nodes at distance > R0 and nodes at distance = 0\n",
    "        t0 = time()\n",
    "        logging.info(f'-- -- Computing neighbors_graph ...')\n",
    "        if similarity in ['He']:\n",
    "            # We must compute the connectivity graph because module\n",
    "            # radius_neighbors_graph looses edges between nodes at zero\n",
    "            # distance\n",
    "            D = radius_neighbors_graph(np.sqrt(Tg), radius=R0,\n",
    "                                       mode='connectivity', metric=d)\n",
    "        elif similarity in ['l1', 'Gauss']:\n",
    "            D = radius_neighbors_graph(Tg, radius=R0, mode='connectivity',\n",
    "                                       metric=d)\n",
    "\n",
    "        logging.info(f'       in {time()-t0} seconds')\n",
    "\n",
    "        # ##############################################\n",
    "        # From distance matrix to list of weighted edges\n",
    "\n",
    "        # Compute lists with origin, destination and value for all edges in\n",
    "        # the graph affinity matrix.\n",
    "        orig_id, dest_id = D.nonzero()\n",
    "\n",
    "        # Since the graph is undirected, we select ordered pairs orig_id,\n",
    "        # dest_id only\n",
    "        self.edgeT_id = list(filter(lambda i: i[0] < i[1],\n",
    "                             zip(orig_id, dest_id)))\n",
    "\n",
    "        # ####################\n",
    "        # Computing Affinities\n",
    "\n",
    "        logging.info(f\"-- -- Computing affinities for {len(self.edgeT_id)}\" +\n",
    "                     \" edges ...\",)\n",
    "        t0 = time()\n",
    "\n",
    "        if similarity == 'He':\n",
    "            # A new self.edgeT_id is returned because the function filters out\n",
    "            # affinity values below th.\n",
    "            self.edgeT_id, self.affinityT = self.he_affinity(Tg, R, g)\n",
    "\n",
    "        elif similarity == 'l1':\n",
    "            self.edgeT_id, self.affinityT = self.l1_affinity(Tg, R, g)\n",
    "\n",
    "        elif similarity == 'Gauss':\n",
    "            self.edgeT_id, self.affinityT = self.l2_affinity(Tg, R, th_gauss)\n",
    "        else:\n",
    "            logging.error(\"computeTsubGraph ERROR: Unknown similarity measure\")\n",
    "\n",
    "        logging.info(f\"      reduced to {len(self.edgeT_id)} edges\")\n",
    "        logging.info(f'      Computed in {time()-t0} seconds')\n",
    "\n",
    "        logging.info((\"-- -- Graph generated with {0} nodes and {1} \" +\n",
    "                      \"edges\").format(self.n_nodes, len(self.edgeT_id)))\n",
    "\n",
    "        return\n",
    "\n",
    "    def he_affinity(self, Tg, R=1, g=1, blocksize=1_000_000):\n",
    "        \"\"\" Compute all Hellinger's affinities between all nodes in the\n",
    "            graph based on the node attribute vectos\n",
    "            It assumes that all attribute vectors are normalized to sum up to 1\n",
    "            Attribute matrix Tg can be sparse\n",
    "\n",
    "            Args:\n",
    "\n",
    "                Tg  :Matrix of probabilistic attribute vectors\n",
    "                R   :Maximum JS distance. Edges at higher distance are removed\n",
    "                g   :Exponent for the finnal affinity mapping\n",
    "        \"\"\"\n",
    "\n",
    "        # ################################\n",
    "        # Compute affinities for all edges\n",
    "\n",
    "        # This is just to make sure that blocksize is an integer, to avoid an\n",
    "        # execution error when used as an array index\n",
    "        blocksize = int(blocksize)\n",
    "\n",
    "        # I take the square root here. This is inefficient if Tg has many\n",
    "        # rows and just af few edges will be computed. However, we can\n",
    "        # expect the opposite (the list of edges involves the most of the\n",
    "        #  nodes).\n",
    "        X = np.sqrt(Tg)\n",
    "\n",
    "        # Divergences are compute by blocks. This is much faster than a\n",
    "        # row-by-row computation, specially when Tg is sparse.\n",
    "        d2_he = []\n",
    "        for i in range(0, len(self.edgeT_id), blocksize):\n",
    "            edge_ids = self.edgeT_id[i: i + blocksize]\n",
    "\n",
    "            # Take the (matrix) of origin and destination attribute vectors\n",
    "            i0, i1 = zip(*edge_ids)\n",
    "\n",
    "            if issparse(Tg):\n",
    "                P = X[list(i0)].toarray()\n",
    "                Q = X[list(i1)].toarray()\n",
    "            else:\n",
    "                P = X[list(i0)]\n",
    "                Q = X[list(i1)]\n",
    "\n",
    "            # Squared Hellinger's distance\n",
    "            # The maximum is used here just to avoid 2-2s<0 due to\n",
    "            # precision errors\n",
    "            s = np.sum(P * Q, axis=1)\n",
    "            d2_he += list(np.maximum(2 - 2 * s, 0))\n",
    "\n",
    "        # #########\n",
    "        # Filtering\n",
    "\n",
    "        # Filter out edges with JS distance above R (divergence above R**2).\n",
    "        edge_id = [z[0] for z in zip(self.edgeT_id, d2_he) if z[1] < R**2]\n",
    "\n",
    "        # ####################\n",
    "        # Computing affinities\n",
    "\n",
    "        # The final affinity values are computed using a transformation the\n",
    "        # states a minimum affinity value equal to zero\n",
    "        affinityT = [(1 - z/R**2)**g for z in d2_he if z < R**2]\n",
    "\n",
    "        return edge_id, affinityT\n",
    "\n",
    "    def l1_affinity(self, Tg, R=1, g=1, blocksize=1_000_000):\n",
    "        \"\"\" Compute all l1's affinities between all nodes in the graph based on\n",
    "            the node attribute vectors\n",
    "            It assumes that all attribute vectors are normalized to sum up to 1\n",
    "            Attribute matrix Tg can be sparse\n",
    "\n",
    "            Args:\n",
    "\n",
    "                Tg  :Matrix of probabilistic attribute vectors\n",
    "                R   :Maximum JS distance. Edges at higher distance are removed\n",
    "                g   :Exponent for the finnal affinity mapping\n",
    "        \"\"\"\n",
    "\n",
    "        # ################################\n",
    "        # Compute affinities for all edges\n",
    "\n",
    "        # This is just to make sure that blocksize is an integer, to avoid an\n",
    "        # execution error when used as an array index\n",
    "        blocksize = int(blocksize)\n",
    "\n",
    "        # I take the square root here. This is inefficient if Tg has many\n",
    "        # rows and just af few edges will be computed. However, we can\n",
    "        # expect the opposite (the list of edges involves the most of the\n",
    "        #  nodes).\n",
    "\n",
    "        # Divergences are compute by blocks. This is much faster than a\n",
    "        # row-by-row computation, specially when Tg is sparse.\n",
    "        d_l1 = []\n",
    "        for i in range(0, len(self.edgeT_id), blocksize):\n",
    "            edge_ids = self.edgeT_id[i: i + blocksize]\n",
    "\n",
    "            # Take the (matrix) of origin and destination attribute vectors\n",
    "            i0, i1 = zip(*edge_ids)\n",
    "            if issparse(Tg):\n",
    "                P = Tg[list(i0)].toarray()\n",
    "                Q = Tg[list(i1)].toarray()\n",
    "            else:\n",
    "                P = Tg[list(i0)]\n",
    "                Q = Tg[list(i1)]\n",
    "\n",
    "            # l1 distance\n",
    "            d_l1 += list(np.sum(np.abs(P - Q), axis=1))\n",
    "\n",
    "        # #########\n",
    "        # Filtering\n",
    "\n",
    "        # Filter out edges with JS distance above R (divergence above R**2).\n",
    "        edge_id = [z[0] for z in zip(self.edgeT_id, d_l1) if z[1] < R**2]\n",
    "\n",
    "        # ####################\n",
    "        # Computing affinities\n",
    "\n",
    "        # The final affinity values are computed using a transformation the\n",
    "        # states a minimum affinity value equal to zero\n",
    "        affinityT = [(1 - z / R)**g for z in d_l1 if z < R]\n",
    "\n",
    "        return edge_id, affinityT\n",
    "\n",
    "    def l2_affinity(self, Tg, R=1, th_gauss=0.1, blocksize=1_000_000):\n",
    "        \"\"\" Compute all l2's affinities between all nodes in the graph based on\n",
    "            the node attribute vectors\n",
    "            It assumes that all attribute vectors are normalized to sum up to 1\n",
    "            Attribute matrix Tg can be sparse\n",
    "\n",
    "            Args:\n",
    "\n",
    "                Tg  :Matrix of probabilistic attribute vectors\n",
    "                R   :Maximum JS distance. Edges at higher distance are removed\n",
    "                :th_gauss:  Similarity threshold All similarity values below\n",
    "                     this threshold are set to zero. This is only for the gauss\n",
    "                     method, the rest of them compute the threshold\n",
    "                     automatically from R).\n",
    "        \"\"\"\n",
    "\n",
    "        # ################################\n",
    "        # Compute affinities for all edges\n",
    "        # This is just to make sure that blocksize is an integer, to avoid an\n",
    "        # execution error when used as an array index\n",
    "        blocksize = int(blocksize)\n",
    "\n",
    "        # I take the square root here. This is inefficient if Tg has many\n",
    "        # rows and just af few edges will be computed. However, we can\n",
    "        # expect the opposite (the list of edges involves the most of the\n",
    "        #  nodes).\n",
    "\n",
    "        # Divergences are compute by blocks. This is much faster than a\n",
    "        # row-by-row computation, specially when Tg is sparse.\n",
    "        d_l2 = []\n",
    "        for i in range(0, len(self.edgeT_id), blocksize):\n",
    "            edge_ids = self.edgeT_id[i: i + blocksize]\n",
    "#             print(len(edge_ids))\n",
    "            # Take the (matrix) of origin and destination attribute vectors\n",
    "            i0, i1 = zip(*edge_ids)\n",
    "        \n",
    "            if issparse(Tg):\n",
    "                P = Tg[list(i0)].toarray()\n",
    "                Q = Tg[list(i1)].toarray()\n",
    "            else:\n",
    "                P = Tg[list(i0)]\n",
    "                Q = Tg[list(i1)]\n",
    "\n",
    "            # l1 distance\n",
    "            \n",
    "#             Real code:\n",
    "#             d_l2 += list(np.sum((P - Q)**2, axis=1))\n",
    "\n",
    "# Jon changed code here:\n",
    "            d_l2 += list(np.sum((P - Q), axis=1))\n",
    "\n",
    "\n",
    "        # #########\n",
    "        # Filtering\n",
    "\n",
    "        # Filter out edges with JS distance above R (divergence above R**2).\n",
    "        edge_id = [z[0] for z in zip(self.edgeT_id, d_l2) if z[1] < R**2]\n",
    "\n",
    "        # ####################\n",
    "        # Computing affinities\n",
    "\n",
    "        # The value of gamma to get min edge weight th_gauss at distance R\n",
    "        gamma = - np.log(th_gauss) / R**2\n",
    "        # Nonzero affinity values\n",
    "        affinityT = [np.exp(-gamma * z) for z in d_l2 if z < R**2]\n",
    "\n",
    "        return edge_id, affinityT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pandas dataframe containing word embeddings...  \n",
    "\n",
    "and normalize each token so all the rows sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'token': tokens}\n",
    "for n in range(wv.vector_size):\n",
    "    data[n] = [wv[token][n] for token in tokens]\n",
    "    \n",
    "df = pd.DataFrame(data = data)\n",
    "\n",
    "\n",
    "# Normalize\n",
    "mat_a = np.matrix(df.iloc[:, 1:])\n",
    "mat_a /= mat_a.sum(axis=1)\n",
    "\n",
    "df.iloc[:, 1:] = mat_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the embedding matrix (ignore the token column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.matrix(df.drop(columns = 'token'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65062"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = SimGraph(matrix)\n",
    "sim.computeGraph(R = 10, similarity='l1')\n",
    "\n",
    "\n",
    "affinity = sim.affinityT\n",
    "\n",
    "edge = sim.edgeT_id\n",
    "\n",
    "len(affinity)\n",
    "\n",
    "len(edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65062, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx = np.array(affinity)\n",
    "xxx[0]\n",
    "\n",
    "\n",
    "yyy = np.array(edge)\n",
    "yyy.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
